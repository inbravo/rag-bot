# trunk-ignore-all(black)
from abc import ABC, abstractmethod

import anthropic
from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from openai import OpenAI

PROMPT_TEMPLATE = """
Basing only on the following context:

{context}

---

Answer the following question: {question}
Avoid to start the answer saying that you are basing on the provided context and go straight with the response.
"""

# amit.dixit@inbravo
# Abstract base class for LLM models
# Defines the interface for invoking models and generating responses
# Concrete implementations for Ollama, GPT, and Anthropic models
# Each subclass implements the invoke method specific to the model's API
# The generate_response method formats the prompt and calls invoke to get the response
# It uses a consistent prompt template across different models.
class LLM(ABC):
    def __init__(self, model_name: str):
        self.model_name = model_name

    @abstractmethod
    def invoke(self, prompt: str) -> str:
        pass

    def generate_response(self, context: str, question: str) -> str:
        prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
        prompt = prompt_template.format(context=context, question=question)
        response_text = self.invoke(prompt)
        return response_text

# amit.dixit@inbravo
# OllamaModel class to interact with Ollama's local LLM models
# Inherits from the abstract LLM base class
# Implements the invoke method to send prompts and receive responses
class OllamaModel(LLM):
    """
    A wrapper class for the Ollama language model, inheriting from the base LLM class.

    Attributes:
        model (Ollama): An instance of the Ollama model initialized with the specified model name.

    Methods:
        __init__(model_name: str):
            Initializes the OllamaModel with the given model name and sets up the Ollama instance.

        invoke(prompt: str) -> str:
            Sends a prompt to the Ollama model and returns the generated response.
    """
    def __init__(self, model_name: str):
        super().__init__(model_name)
        self.model = Ollama(model=model_name)

    def invoke(self, prompt: str) -> str:
        return self.model.invoke(prompt)

# amit.dixit@inbravo
# GPTModel class to interact with OpenAI's GPT models
# Inherits from the abstract LLM base class
# Implements the invoke method to send prompts and receive responses
# Uses the openai Python SDK to communicate with the OpenAI API
# The invoke method formats the prompt and extracts the response text
# The generate_response method is inherited from the LLM base class
# It uses a consistent prompt template for generating responses ccross different models.
class GPTModel(LLM):
    """
    GPTModel is a class that represents a wrapper around the OpenAI GPT model, allowing for interaction
    with the model using prompts and returning generated responses.

    Attributes:
        client (OpenAI): An instance of the OpenAI client used to interact with the GPT model.

    Methods:
        __init__(model_name: str, api_key: str):
            Initializes the GPTModel with the specified model name and API key.

        invoke(prompt: str) -> str:
            Sends a prompt to the GPT model and returns the generated response.

            Args:
                prompt (str): The input prompt to be sent to the GPT model.

            Returns:
                str: The response generated by the GPT model.
    """
    # Constructor to initialize the GPTModel with model name and API key
    def __init__(self, model_name: str, api_key: str):
        super().__init__(model_name)
        self.client = OpenAI(api_key=api_key)

    # Method to invoke the GPT model with a given prompt
    def invoke(self, prompt: str) -> str:
        messages = [
            # {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            max_tokens=150,
            n=1,
            stop=None,
            temperature=0.7,
        )
        return response.choices[0].message.content.strip()


# amit.dixit@inbravo
# AnthropicModel class to interact with Anthropic's Claude models
# Inherits from the abstract LLM base class
# Implements the invoke method to send prompts and receive responses
# Uses the anthropic Python SDK to communicate with the Anthropic API
# The invoke method formats the prompt and extracts plain text from the response
# The generate_response method is inherited from the LLM base class
# It uses a consistent prompt template for generating responses
# across different models.
class AnthropicModel(LLM):
    """
    A class representing an interface to the Anthropic language model.

    This class is designed to interact with the Anthropic API to generate
    responses based on a given prompt. It extends the base `LLM` class.

    Attributes:
        client (anthropic.Anthropic): The Anthropic API client instance.

    Methods:
        __init__(model_name: str, api_key: str):
            Initializes the AnthropicModel with the specified model name and API key.

        invoke(prompt: str) -> str:
            Sends a prompt to the Anthropic API and returns the generated response
            as plain text.
    """
    # Constructor to initialize the AnthropicModel with model name and API key
    def __init__(self, model_name: str, api_key: str):
        super().__init__(model_name)
        self.client = anthropic.Anthropic(api_key=api_key)

    # Method to invoke the Anthropic model with a given prompt
    def invoke(self, prompt: str) -> str:
        messages = [{"role": "user", "content": [{"type": "text", "text": prompt}]}]
        response = self.client.messages.create(
            model=self.model_name, max_tokens=1000, temperature=0.7, messages=messages
        )
        # Extract the plain text from the response content
        text_blocks = response.content
        plain_text = "\n".join(
            block.text for block in text_blocks if block.type == "text"
        )
        return plain_text